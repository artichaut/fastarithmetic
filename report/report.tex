\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,amsthm,amsopn}
\usepackage{mathrsfs}
\usepackage{graphicx}
\usepackage{hyperref}
%\usepackage{tikz}
\usepackage{array}
%\usepackage[top=1cm,bottom=1cm]{geometry}
%\usepackage{listings}
%\usepackage{xcolor}
\usepackage{bm}
\usepackage{bbm}

% Création des labels Théorème, Lemme, etc...

\newtheoremstyle{break}%
{}{}%
{\itshape}{}%
{\bfseries}{}%  % Note that final punctuation is omitted.
{\newline}{}

\theoremstyle{break}
\newtheorem{thm}{Théorème}[section]
\newtheorem{lm}[thm]{Lemme}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollaire}

\theoremstyle{definition}
\newtheorem{defi}[thm]{Définition}
\newtheorem{ex}[thm]{Exemple}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remarque}

% Raccourcis pour les opérateurs mathématiques (les espaces avant-après sont
% modifiés pour mieux rentrer dans les codes mathématiques usuels)
% \deg already exists !!
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\Card}{Card}
\DeclareMathOperator{\Vect}{Vect}

\DeclareBoldMathCommand{\bxi}{\xi}
\DeclareBoldMathCommand{\bupsilon}{\upsilon}
\DeclareBoldMathCommand{\bzeta}{\zeta}

% Nouvelles commandes
\newcommand{\ps}[2]{\langle#1,#2\rangle}
\newcommand{\ent}[2]{[\![#1,#2]\!]}

% opening
\title{Internship report}
\author{Édouard \textsc{Rousseau}\\Internship supervised by Luca \textsc{De Feo}}



\begin{document}

\maketitle

\begin{abstract}

  This internship took place during July and August 2016, the aim was to study
  the algorithms presented in \cite{DDS14}, and to implement them in Nemo
  \cite{Nemo}, a computer algebra package for the Julia \cite{Julia} programming
  language. This paper intends to compare the Julia and the C implementations of
  the code, in terms of speed, but also genericity and easyness to write and
  read such code.

\end{abstract}

\tableofcontents

\clearpage

\section{Julia}
\subsection{Overview of Julia's caracteristics}
Julia is a free and open-source, high-level programming language developed since 2012, with dynamic
type system and high-performance. It is a compiled language, with a
\emph{just-in-time} (jit) compilation. It means that when you write a
function, e.g. 
\begin{verbatim}
julia> function myFunction(n)
       return n^3+n+2
       end
\end{verbatim}
your function will be compiled during your very first call. After this first
call, it will be much faster to run the compiled function
\begin{verbatim}
julia> myFunction(BigInt(2)^300)
\end{verbatim}
than the expression below.
\begin{verbatim}
julia> (BigInt(2)^300)^3 + BigInt(2)^300 + 2
\end{verbatim}
In order to optimize the code, the compiler creates a new function for each
type of input that you can think of when calling myFunction. For example, in the
code 
below, Julia will compile myFunction three times, which will result in
three functions, each one of them optimized for \textbf{Int64} (the default type
of $17$), \textbf{Float64}, or \textbf{BigInt}.
\begin{verbatim}
julia> myFunction(17)
4932
julia> myFunction(17.)
4932.0
julia> myFunction(BigInt(17))
4932
\end{verbatim}
Julia is a very easy-to-learn language, I had never seen Julia code before the 
internship, and I am not an expert in computer science, but the writing part of
the code was not the hardest. 
The code written in FastArithmetic.jl is
very generic, thanks to the type system of Julia, and works for fields
$\mathbb{F}_{p^n}$ with a small $p$ as well as large $p$. More importantly, the
\emph{jit} compiler creates a function for each kind of $p$. Indeed the elements
of $\mathbb{F}_{p^n}$ have type \textbf{fq\_nmod} for $p$ small or \textbf{fq}
for $p$ large, so one can write only one function and have two compiled
functions : one for each type, which will be optimized by the compiler for this type.

\subsection{Nemo}

Nemo is a computer algebra package of Julia, you can install it with 
\begin{verbatim}
julia> using Nemo
\end{verbatim}
and test it with the following.
\begin{verbatim}
julia> Pkg.test(``Nemo'')
\end{verbatim}
Nemo is based on C/C++ libraries such as Flint, Antic, Pari, etc. and is also
written in Julia. It provides a lot of features, and all the work of this
internship is based on Nemo. Indeed, we work with Nemo finite fields and Nemo
polynomials over these fields. \textbf{fq}, \textbf{fq\_nmod}, and
\textbf{fq\_nmod\_poly} (the type of the polynomials over $\mathbb{F}_{p^n}$) are
all Nemo types. You can learn more in Nemo's manual, which is very well
documented.

\section{A bit of theory}
\subsection{General background}
In most of the computer algebra system, like Sage, Magma, Flint, etc., it is possible to work with finite
fields and their algebraic closure. But the algorithms used to deal with this
last object often rely on linear algebra, and the cost is at least quadratic in
the degree of the extension you are dealing with. The algorithms presented in
\cite{DDS14} rely on polynomial arithmetic, and have a quasi-linear cost for most of the algorithms needed to
compute in the algebraic closure of a finite field. In this section, we will
introduce (briefly) the tools used in the algorithms, and explain how it works.
The most important result is the \emph{transposition principle}, which is an
algorithmic proposition that guarantees that if you have a linear algorithm
performing a matrix-vector product $v\mapsto Mv$, you can \emph{tranpose} it to
obtain an algorithm, which has essentialy the same cost, performing the transposed matrix-vector product $v\mapsto
M^tv$. But we first need to talk about \emph{duality}, because the transposition
principle also change the base you are working with.
\subsection{Trace and duality}
Let $k$ be a finite field, and  $E,F$ be two $k$-vector spaces with the same
finite dimension $\dim E = \dim F < \infty$. Let also
$\ps{\cdot}{\cdot}:E\times F\rightarrow k$ be a non-degenerate bilinear form. Then,
for any basis $\bxi=(\xi_i)_i$ of $E$, their exists a unique basis
$\bxi^*=(\xi_j^*)_j$ of $F$
such that $\forall (i,j),\ps{\xi_i}{\xi_j^*}=\delta_{i,j}$ where $\delta_{i,j}$ is
the \textsc{Kronecker} symbol and $\delta_{i,j}=1$ if $i=j$ and 0 otherwise.
The base $\bxi^*$ is called the \emph{dual basis} of $\bxi$.
Equally, let $E',F'$ be two other $k$-vector spaces with $\dim E' = \dim F' <
\infty$ and $\ps{\cdot}{\cdot}':E'\times F'\rightarrow k$ an other non-degenerate
bilinear form. Then, for any linear map $u:E\rightarrow E'$, there exists a
unique map $u^t:F'\rightarrow F$ such that $\forall (a,b)\in E\times
F',\ps{u(a)}{b}'=\ps{a}{u^t(b)}$. This map is called the \emph{dual} map of $u$,
with respect to $\ps{\cdot}{\cdot}$ and $\ps{\cdot}{\cdot}'$.

It's now time to speak about the \emph{trace form}, it will be our candidate to be 
$\ps{\cdot}{\cdot}$, the non-degenerate bilinear form. Let $P$ be an irreducible
polynomial of $k[X]$, $K=k[X]/(P)$ be a finite
extension of $k$, and $a\in K$. We denote by $\mu_a$ the application of
multiplication-by-a, $\mu_a:K\rightarrow K, x\mapsto ax$. $K$ is a $k$-vector
space and $\mu_a$ is an endomorphism of $K$. But, for all $a,b\in K$, we also denote $\tau_P(ab)$ to be the trace of the endomorphism $\mu_{ab}$, and we define
$\ps{a}{b}_P=\tau_P(ab)$. This defines a non-degenerate bilinear form on $K\times
K$. Thus, if $\bxi$ is a basis of $K$, one can obtain a dual basis $\bxi^*$ with
respect to the bilinear form $\ps{\cdot}{\cdot}_P$. Hence, if $a\in K$ has the
coordinates $(a_i)_i$ in the basis $\bxi^*$, these coordinates are given by
$a_i=\ps{\xi_i}{a}$, so $a=\sum a_i\xi_i^*=\sum\ps{\xi_i}{a}\xi_i^*$. This
formula will be important for the change-of-basis algorithms.

\subsection{Transposition principle}
Let, as in the previous section, $\bxi$ be a basis of $E$, and $\bxi^*$ the
basis of $F$ which is dual to $\bxi$. Let also $\bupsilon$ and $\bupsilon^*$ be a
basis of $E'$ and its dual basis of $F'$. If $M$ is the matrix of $u$ in the
bases $(\bxi,\bupsilon)$, then the matrix of $u^t$ in the bases
$(\bupsilon^*,\bxi^*)$ is $M^t$. Given an algorithm to compute a linear map
$u:E\rightarrow E'$ in the bases $(\bxi,\bupsilon)$, the transposition principle
enable one to compute the dual map $u^t:F'\rightarrow F$ in the bases
$(\bupsilon^*,\bxi^*)$. The costs of the two algorithms only differ by a
constant. Roughly speaking, transposing an algorithm consist on transposing all
the subroutines and inverse their orders.
\begin{center}
  Maybe place here a tabular with drawing \& algorithmic result in both
  transposed and non-transposed mode. 
\end{center}
\subsection{The algorithms}
\section{Julia/Nemo in practice}
Since Julia is easy-to-write, I hope it's also
easy-to-read, if not, I think that it is due to my lack of experience in
software programing, rather than to Julia itself. You can find the code
at
https://gitbub.com/edouardRousseau/FastArithmetic.jl. What's more, it is really
easy to install personal packages, you just have to clone my repository
inside Julia, using
\begin{verbatim}
julia> Pkg.clone(``https://github/edouardRousseau/FastArithmetic.jl'') 
\end{verbatim}
and you can also test the package easily by running the following.
\begin{verbatim}
julia> Pkg.test(``FastArithmetic'') 
\end{verbatim}
You may want to update the package, because the code is very suceptible to change, so just use
\begin{verbatim}
julia> Pkg.update()
\end{verbatim}
and you will be ready to start again. 
\subsection{Benchmarks}
We will now look at the speed of the code in Julia, and compare it with
the speed in C. All the benchmarks in Julia were realised with the
@benchmark macro available in the BenchmarkTools package. This macro lets you
benchmark any function with 

\begin{verbatim}
julia>@benchmark 1+1
BenchmarkTools.Trial: 
  samples:          10000
  evals/sample:     1000
  time tolerance:   5.00%
  memory tolerance: 1.00%
  memory estimate:  0.00 bytes
  allocs estimate:  0
  minimum time:     3.00 ns (0.00% GC)
  median time:      3.00 ns (0.00% GC)
  mean time:        3.04 ns (0.00% GC)
  maximum time:     10.00 ns (0.00% GC)
\end{verbatim}
which runs the desired function and measures the time it takes. It's possible
to choose all the parameters of the benchmark and you can use different
estimators. For the following benchmarks, we use the median time, because it
is more stable than the mean time and still give an idea of the average time
the function will take. The stability we are talking about is linked with the
memory management in Julia. Indeed, in Julia, we don't have to concern about
memory, and the variables we assign are cleared by the Garbage Collector
(GC). The time Julia pass in the GC is not very clear nor predictible, and the
operations in the GC take a significant time. That's why the time taken by a
function can change between two samples, depending on the time spent in the GC.
By default, before every benchmark, the garbage if automaticaly collected, but
there is still a bit of unstabiliy.
Since all the functions written in FastArithmetic.jl depends on at least one
polynomial $P$, sometimes also $Q$ and $R=P\odot Q$, we let $m=\deg (P)$ grow
from $1$ to $200$ and benchmark the functions for all these values of $m$. We
choose $\deg (Q)=m+1$, and both $P$ and $Q$ are irreducible polynomials.
Irreducible polynomials of degree $1$ to $201$ were precomputed, by chosing
random elements in $\mathbb{F}_{p}[X]$, in order to
always use the same list and to be sure that we are not using special
polynomials that could be speeding up the functions (like sparse
polynomials). In the arrays, the speed of
the C code is set to $1.00$, and smaller is better. The speed of 
\textbf{monomialToDual} and of \textbf{dualToMonomial} are the same, either in C
or Julia, so we show the results only for the first.

\begin{center}
  \begin{tabular}[here]{c}
    \textbf{mulmod} \\
    \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $15.47$\\
   $51$ & $12.84$\\
   $104$ & $17.04$\\
   $140$ & $16.43$\\
   $187$ & $14.78$\\
    \end{tabular}
  \end{tabular}
\end{center}

\begin{center}
\begin{tabular}[here]{ccc}
\textbf{monomialToDual} & \textbf{mulModT} & \textbf{embed} \\

  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $5.12$\\
   $51$ & $1.14$\\
   $104$ & $0.56$\\
   $140$ & $0.44$\\
   $187$ & $0.38$\\

  \end{tabular}

  &
  
  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $286$\\
   $51$ & $2.6\times 10^3$\\
   $104$ & $6.8\times 10^3$\\
   $140$ & $1.17\times 10^4$\\
   $187$ & $2\times 10^4$\\

  \end{tabular}

  &

  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $31.30$\\
   $51$ & $37.17$\\
   $104$ & $35.29$\\
   $140$ & $33.87$\\
   $187$ & $30.87$\\

  \end{tabular}\\

    \textbf{project} & \textbf{phi1} & \textbf{phi2} \\

  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $32.77$\\
   $51$ & $30.64$\\
   $104$ & $26.64$\\
   $140$ & $24.24$\\
   $187$ & $20.64$\\

  \end{tabular}

  &
 
  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $25.83$\\
   $51$ & $29.55$\\
   $104$ & $22.53$\\
   $140$ & $22.94$\\
   $187$ & $22.12$\\

  \end{tabular}

  &
 
  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $2.56$\\
   $51$ & $102$\\
   $104$ & $187$\\
   $140$ & $309$\\
   $187$ & $449$\\

  \end{tabular}
\end{tabular}
\end{center}


\clearpage
\bibliographystyle{unsrt}
\bibliography{biblio}
\end{document}
