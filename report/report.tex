%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% # TO DO LIST
% 
% * read again and put all the citations needed when something is stated 
%   without demonstration (a lot of times...)
% * find a reference for the duality theory (other that Bourbaki ?)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,amsthm,amsopn}
\usepackage{mathrsfs}
%\usepackage{stmaryrd} % to get <--| \mapsfrom
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{tikz}
\usepackage{array}
%\usepackage[top=1cm,bottom=1cm]{geometry}
%\usepackage{listings}
%\usepackage{xcolor}
\usepackage{bm}
\usepackage{bbm}

% Création des labels Théorème, Lemme, etc...

\newtheoremstyle{break}%
{}{}%
{\itshape}{}%
{\bfseries}{}%  % Note that final punctuation is omitted.
{\newline}{}

\theoremstyle{break}
\newtheorem{thm}{Théorème}[section]
\newtheorem{lm}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollaire}

\theoremstyle{definition}
\newtheorem{defi}[thm]{Définition}
\newtheorem{ex}[thm]{Exemple}

\theoremstyle{remark}
%\newtheorem{rem}[thm]{Remarque}

% Raccourcis pour les opérateurs mathématiques (les espaces avant-après sont
% modifiés pour mieux rentrer dans les codes mathématiques usuels)
% \deg already exists !!
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\Card}{Card}
\DeclareMathOperator{\Vect}{Vect}
\DeclareMathOperator{\rev}{rev}
\DeclareMathOperator{\rem}{rem}

\DeclareBoldMathCommand{\bxi}{\xi}
\DeclareBoldMathCommand{\bupsilon}{\upsilon}
\DeclareBoldMathCommand{\bzeta}{\zeta}

% Nouvelles commandes
\newcommand{\ps}[2]{\langle#1,#2\rangle}
\newcommand{\ent}[2]{[\![#1,#2]\!]}
\newcommand{\psdot}{\ps{\cdot}{\cdot}}
\newcommand{\remt}{\rem^t}

% opening
\title{Internship report}
\author{Édouard \textsc{Rousseau}\\Internship supervised by Luca \textsc{De 
Feo}}

\usepackage{tikz-cd}
\usetikzlibrary{matrix, calc, arrows}



\begin{document}

\maketitle

\begin{abstract}

  This internship took place during July and August 2016, the aim was to study
  the algorithms presented in \cite{DeDoSc14}, and to implement them in Nemo
  \cite{Nemo}, a computer algebra package for the Julia \cite{Julia} programming
  language. This paper intends to compare the Julia and the C implementations of
  the code, in terms of speed, but also genericity and easyness to write and
  read such code.

\end{abstract}

\tableofcontents

\clearpage

\section{Julia}
\subsection{Overview of Julia's caracteristics}
Julia is a free and open-source, high-level programming language developed since 
2012, with dynamic
type system and high-performance. It is a compiled language, with a
\emph{just-in-time} (jit) compilation. It means that when you write a
function, e.g. 
\begin{verbatim}
julia> function myFunction(n)
       return n^3+n+2
       end
\end{verbatim}
your function will be compiled during your very first call. After this first
call, it will be much faster to run the compiled function
\begin{verbatim}
julia> myFunction(BigInt(2)^300)
\end{verbatim}
than the expression below.
\begin{verbatim}
julia> (BigInt(2)^300)^3 + BigInt(2)^300 + 2
\end{verbatim}
In order to optimize the code, the compiler creates a new function for each
type of input that you can think of when calling myFunction. For example, in the
code 
below, Julia will compile myFunction three times, which will result in
three functions, each one of them optimized for \textbf{Int64} (the default type
of $17$), \textbf{Float64}, or \textbf{BigInt}.
\begin{verbatim}
julia> myFunction(17)
4932
julia> myFunction(17.)
4932.0
julia> myFunction(BigInt(17))
4932
\end{verbatim}
Julia is a very easy-to-learn language, I had never seen Julia code before the 
internship, and I am not an expert in computer science, but the writing part of
the code was not the hardest. 
The code written in FastArithmetic.jl is
very generic, thanks to the type system of Julia, and works for fields
$\mathbb{F}_{p^n}$ with a small $p$ as well as large $p$. More importantly, the
\emph{jit} compiler creates a function for each kind of $p$. Indeed the elements
of $\mathbb{F}_{p^n}$ have type \textbf{fq\_nmod} for $p$ small or \textbf{fq}
for $p$ large, so one can write only one function and have two compiled
functions : one for each type, which will be optimized by the compiler for this 
type.

\subsection{Nemo}

Nemo is a computer algebra package of Julia, you can install it with 
\begin{verbatim}
julia> using Nemo
\end{verbatim}
and test it with the following.
\begin{verbatim}
julia> Pkg.test(``Nemo'')
\end{verbatim}
Nemo is based on C/C++ libraries such as Flint, Antic, Pari, etc. and is also
written in Julia. It provides a lot of features, and all the work of this
internship is based on Nemo. Indeed, we work with Nemo finite fields and Nemo
polynomials over these fields. \textbf{fq}, \textbf{fq\_nmod}, and
\textbf{fq\_nmod\_poly} (the type of the polynomials over $\mathbb{F}_{p^n}$) 
are
all Nemo types. You can learn more in Nemo's manual, which is very well
documented.

\section{A bit of theory}
\subsection{General background}
In most of the computer algebra system, like Sage, Magma, Flint, etc., it is 
possible to work with finite
fields and their algebraic closure. Let $p$ be a prime number and
$k=\mathbb{F}_p$ the field with $p$ elements, the algebraic closure of $k$ is
infinite and is $\bar k = \cup_{i\geq 0} \mathbb{F}_{p^i}$, dealing with $\bar
k$ means being able to compute the finite fields $\mathbb{F}_{p^i}$, and also
being able to embed small fields into large ones, or project elements of large
fields into smaller ones (when possible).

These algorithms often rely on linear algebra, and the cost is at least 
quadratic in
the degree of the extension you are dealing with. The algorithms presented in
\cite{DeDoSc14} rely on polynomial arithmetic, and have a quasi-linear cost for 
most of the algorithms needed to
compute in the algebraic closure of a finite field. In this section, we will
introduce (briefly) the tools used in the algorithms, and explain how they work.
The most important result is the \emph{transposition principle}, which is an
algorithmic proposition that guarantees that if you have a linear algorithm
performing a matrix-vector product $v\mapsto Mv$, you can \emph{tranpose} it to
obtain an other algorithm, which has essentialy the same cost, performing the 
transposed matrix-vector product $v\mapsto
M^tv$. But we first need to talk about \emph{duality}, because the transposition
principle also change the base you are working with.
\subsection{Trace and duality}
Let $k$ be a finite field, and  $E,F$ be two $k$-vector spaces with the same
finite dimension $\dim E = \dim F < \infty$. Let also
$\ps{\cdot}{\cdot}:E\times F\rightarrow k$ be a non-degenerate bilinear form. 
Then,
for any basis $\bxi=(\xi_i)_i$ of $E$, their exists a unique basis
$\bxi^*=(\xi_j^*)_j$ of $F$
such that $\forall (i,j),\ps{\xi_i}{\xi_j^*}=\delta_{i,j}$ where $\delta_{i,j}$ 
is
the \textsc{Kronecker} symbol and $\delta_{i,j}=1$ if $i=j$ and 0 otherwise.
The base $\bxi^*$ is called the \emph{dual basis} of $\bxi$.
Equally, let $E',F'$ be two other $k$-vector spaces with $\dim E' = \dim F' <
\infty$ and $\ps{\cdot}{\cdot}':E'\times F'\rightarrow k$ an other 
non-degenerate
bilinear form. Then, for any linear map $u:E\rightarrow E'$, there exists a
unique map $u^t:F'\rightarrow F$ such that $\forall (a,b)\in E\times
F',\ps{u(a)}{b}'=\ps{a}{u^t(b)}$. This map is called the \emph{dual} map of $u$,
with respect to $\ps{\cdot}{\cdot}$ and $\ps{\cdot}{\cdot}'$.

It's now time to speak about the \emph{trace form}, it will be our candidate to 
be 
$\ps{\cdot}{\cdot}$, the non-degenerate bilinear form. Let $P$ be an irreducible
polynomial of $k[x]$ of degree $m$, $K=k[x]/(P)$ be a finite
extension of $k$, and $a\in K$. We denote by $\mu_a$ the application of
multiplication-by-a, $\mu_a:K\rightarrow K, b\mapsto ab$. $K$ is a $k$-vector
space of dimension $m$ and $\mu_a$ is an endomorphism of $K$. But, for all 
$a,b\in K$, we also denote $\tau_P(ab)$ to be the trace of the endomorphism 
$\mu_{ab}$, and we define
$\ps{a}{b}_P=\tau_P(ab)$. This defines a non-degenerate bilinear form on 
$K\times
K$. Thus, if $\bxi$ is a basis of $K$, one can obtain a dual basis $\bxi^*$ with
respect to the bilinear form $\ps{\cdot}{\cdot}_P$. Hence, if $a\in K$ has the
coordinates $(a_i)_i$ in the basis $\bxi^*$, these coordinates are given by
$a_i=\ps{\xi_i}{a}$, so $a=\sum a_i\xi_i^*=\sum\ps{\xi_i}{a}\xi_i^*$. This
formula will be important for the change-of-basis algorithms.

Let now $Q$ be an irreducible polynomial of $k[y]$ of degree $n$, with $n$
coprime to $m$, then $L=k[x,y]/(P,Q)\cong \mathbb{F}_{p^{mn}}$ is also a finite 
extension of $k$, of
degree $mn$. Since $L$ is a $k$-vector space (of dimension $mn$), it is again
possible to construct the trace $\tau_{P,Q}$ of an element of $L$, and we define 
the same way
a non-degenerate bilinear form $\psdot_{P,Q}$ on $L\times L$. If
$\bxi=(x^i)_{0\leq i\leq m-1}$ and $\bupsilon=(y^j)_{0\leq j \leq n-1}$ are the
monomial basis of respectively $k[x]/(P)$ and $k[y]/(Q)$, then
$\bxi\otimes\bupsilon=(x^iy^j)_{0\leq i \leq m-1,0\leq j \leq n-1}$ is the
cannonical monomial basis of $L$. But, if $\bxi^*$ and $\bupsilon^*$ are
respectively the dual basis of $\bxi$ with respect to $\tau_P$ and the dual
basis of $\bupsilon$ with respect to $\tau_Q$, $\bxi\otimes\bupsilon^*$, 
$\bxi^*\otimes\bupsilon$, and $\bxi^*\otimes\bupsilon^*$, are also bases of $L$.
What's more, $\bxi\otimes\bupsilon$ and  $\bxi^*\otimes\bupsilon^*$ are dual
with respect to $\tau_{P,Q}$, and so are $\bxi\otimes\bupsilon^*$ and
$\bxi^*\otimes\bupsilon$.

Constructing
$\mathbb{F}_{p^{mn}}$ this way leads to a bivariate reprentation of the
elements. We prefer an univariate representation, because the algorithm we have
in this case are more efficient, even for simple operations such as addition or
multiplication. Hence, we can represent $\mathbb{F}_{p^{mn}}$ by $k[z]/(R)$
where $R=P\odot Q$ is the composed product of $P$ and $Q$. It means that if
$(p_i)_{0\leq i \leq m-1}$ and $(q_j)_{0\leq j\leq n-1}$ are respectively the
roots of $P$ and $Q$ in a algebraic closure of $k$, $R$ is the polynomial whose
roots are the products $p_iq_j$. So we have embeddings $\varphi_x$, $\varphi_y$
and an isomorphism $\Phi$ of the form :
\[
\begin{array}{ccccc}
  &\varphi_x: & k[x]/(P) & \to & k[z]/(R)\\[2mm]
  & \varphi_y: & k[y]/(Q) & \to & k[z]/(R)\\[2mm]
  \text{and}& \Phi:&  A=k[x,y]/(P,Q) & \to & k[z]/(R) \\
  & &  xy & \mapsto & z.
\end{array}
\]

We will provide algorithms for $\varphi_x$, $\varphi_y$ and $\Phi$ in section
\ref{sec-algo}.

\subsection{Transposition principle}
Let, as in the previous section, $\bxi$ be a basis of $E$, and $\bxi^*$ the
basis of $F$ which is dual to $\bxi$. Let also $\bupsilon$ and $\bupsilon^*$ be 
a
basis of $E'$ and its dual basis of $F'$. If $M$ is the matrix of a linear map
$u:E\rightarrow E'$ in the
bases $(\bxi,\bupsilon)$, then the matrix of the linear map $u^t:F'\rightarrow 
F$ in the bases
$(\bupsilon^*,\bxi^*)$ is $M^t$. Given an algorithm to compute a linear map
$u:E\rightarrow E'$ in the bases $(\bxi,\bupsilon)$, the transposition principle
says that you can transpose it to obtain a new algorithm computing the dual map 
$u^t:F'\rightarrow F$ in the bases
$(\bupsilon^*,\bxi^*)$. The costs of the two algorithms only differ by a
constant. Roughly speaking, transposing an algorithm consists on transposing all
the subroutines and inverse their orders.

Let $A$ be an algorithm taking $x_1$ and $x_2$ as input, and computing 
$y_1=ax_1+bx_2$ and $y_2=cx_1+dx_2$. $A$ is linear and we obtain its 
transposition $A^t$ by reversing all the arrows and exchanging $\times$ and $+$ 
in the graph representing $A$. We obtain an algorithm that takes $y_1$ and 
$y_2$ as input and compute $x_1=ay_1+cy_2$ and $x_2=by_1+dy_2$.

\begin{tikzpicture}
  \matrix[matrix of math nodes,column sep={60pt,between origins},row
    sep={60pt,between origins},nodes={asymmetrical rectangle}] (s)
  {
    &|[name=x1]| x_1 &|[name=1]| \times &|[name=2]| + & |[name=y1]| y_1\\
    %
    &|[name=x2]| x_2 &|[name=3]| \times &|[name=4]| + &|[name=y2]| y_2 \\
  };
  \draw[->,>=latex] (1) edge node[auto] {\(a\)} (2)
  			(3) edge node[auto] {\(d\)} (4)
            (1) edge (4)
            (3) edge (2)
            (x1) edge (1)
            (x2) edge (3)
            (2) edge (y1)
            (4) edge (y2)
            ;
            \draw (1.5,0.3) node {$b$}
            (1.5,-0.3) node {$c$}
            ;
	    \draw[->,>=latex,gray,dotted,thick] (1,-1.5) -- (1,-2.5);
	    \draw[gray] (3,-2) node {transposition principle};
\end{tikzpicture}

\begin{tikzpicture}
  \matrix[matrix of math nodes,column sep={60pt,between origins},row
    sep={60pt,between origins},nodes={asymmetrical rectangle}] (s)
  {
    &|[name=x1]| x_1 &|[name=1]| + &|[name=2]| \times & |[name=y1]| y_1\\
    %
    &|[name=x2]| x_2 &|[name=3]| + &|[name=4]| \times &|[name=y2]| y_2 \\
  };
  \draw[->,>=latex] (2) edge node[auto] {\(a\)} (1)
  		(4) edge node[auto] {\(d\)} (3)
            (4) edge (1)
            (2) edge (3)
            (1) edge (x1)
            (3) edge (x2)
            (y1) edge (2)
            (y2) edge (4)
            ;
            \draw (1.5,0.3) node {$b$}
            (1.5,-0.3) node {$c$};
\end{tikzpicture}

Let us introduce some operations and their transposes. Let $k$ be a field and 
$k[x]_m$ the vector space of polynomials of degree at most $m$, the reversal 
operator of $k[x]_m$ is the map such that

\[  
\begin{array}{rccc}
  \textbf{rev}(\cdot,m): & k[x]_m & \to & k[x]_m \\
   & Q & \mapsto & x^mQ(1/x)
\end{array}.
\]
We can see that \textbf{rev} is its own transpose, indeed \textbf{rev} is a 
linear map and its matrix in the cannocial basis of $k[x]_m$ is symmetric : it 
is the matrix with $1$ on the anti-diagonal. Let now $P\in k[x]$ be a monic 
polynomial of degree $m$ and $n\in\mathbb{N}$ an integer, we denote by 
$\textbf{rem}(\cdot,P,n)$ the remainder by $P$ with length $n$, defined by 
\[  
\begin{array}{rccc}
  \textbf{rem}(\cdot,P,n): & k[x]_n & \to & k[x]_{m-1} \\
   & Q & \mapsto & Q\mod P
\end{array}.
\]

As you can see in \cite{BoLeSc03}, $\textbf{rem}^t(\cdot,P,n)$ is the linear 
sequence extension : it takes as input the $m$ first elements of a linear 
recurring sequence and compute the $n+1$ first using $P$ as the minimal 
polynomial of the sequence.

\subsection{The algorithms}
\label{sec-algo}
All the algorithms described here, and their complexity analysis, have been
discussed in \cite{DeDoSc14}, that's why we won't show all the pseudo-codes but
rather try to explain how they work and on which result they are based. The 
complexities will also be given without any proofs. 

As we saw in the previous section, transposing an algorithm result in a change
of basis. Thus, it is important to be able to go from a basis to another. Here
again, $k$ is a finite field and, $P$ an irreducible monic polynomial of $k[x]$, 
and
$K=k[x]/(P)$ is a finite extension of $k$. We will be considering two bases of
$K$ : the monomial basis $\bxi=(x^i)_{0\leq i \leq m-1}$ (where $m$ is the
degree of $P$), and its dual basis $\bxi^*$ with respect to $\psdot_P$.

\paragraph{Change of basis.}The two
first algorithms allow us to juggle the monomial and the dual basis. They are
both based on a lemma which describe the generating series of the traces
$\tau_P(ax^i)_{0\leq i \leq m-1}$, where $a\in K$. 
\begin{lm}
  \label{lm-basis}
  Let $a\in K$ be an element of $K$, in the formal power series ring $k[[x]]$,
  we have:
  \[
    \sum_{i\geq 0}\tau_P(ax^i)x^i=\frac{\rev(P'a\mod P,m)}{\rev(P,m+1)}.
  \]
\end{lm}
Going from the monomial basis to the dual basis can be done by computing
$\rev(P'a\mod P,m)\mod x^m$, $(\rev(P,m+1)\mod x^m)^{-1}$, and the product of
them. You obtain, by lemma \ref{lm-basis} :
\[
  \frac{\rev(P'a\mod P,m)}{\rev(P,m+1)}\mod x^m=\sum_{i=0}^{m-1}\tau_P(ax^i)x^i.
\]
Since we said that
$(\tau_P(ax^i))_{0\leq i \leq m-1}$ are the coordinates of $a$ in the dual basis
$\bxi^*$, taking the coefficient of the polynomial
$\sum_{i=0}^{m-1}\tau_P(ax^i)x^i$ gives a solution to go from the monomial basis
to the dual basis, that can be written as follows.
\newline

\textbf{MonomialToDual}(a, P)\\
\textbf{Input} $\textbf{a}=(a_i)_{0\leq i\leq m-1}\in k^m$, $P$ an irreducible
monic polynomial of degree~$m$.\\
\textbf{Output} $(\tau_P(ax^i))_{0\leq i \leq m-1}$, with $a=\sum_{0\leq i
\leq m-1}a_ix^i$
\begin{enumerate}
  \item $T=1/\rev(P,m+1) \mod x^m$
  \item $b=\rev(P'a=\sum_{0\leq i\leq m-1}a_ix^i \mod P,m)T\mod x^m$
  \item \textbf{return} (coefficient($b,x^i))_{0\leq i \leq m-1}$
\end{enumerate}

Obtaining the coefficients in the monomial basis $\bxi$
knowing the coefficients in the dual basis $\bxi^*$ is done using the same
equality, but starting from the other half. So, computing $\sum_{0\leq i \leq
m-1}\tau_P(ax^i)x^i \mod x^m$, $\rev(P,m+1)$, and multiplying them gives, by
lemma \ref{lm-basis} again : 
\[
\rev(P,m+1)\sum_{0\leq i \leq m-1}\tau_P(ax^i)x^i \mod x^m = \rev(P'a\mod P,m) 
\mod x^m.
\]
Then, applying $\rev(\cdot,m)$ and multiplying by $(P')^{-1}\mod P$ gives $a\mod
P$, so taking the coefficients of $a$ gives the coefficients in the monomial
basis $\bxi$.
\newline

\textbf{DualToMonomial}(b, P)\\
\textbf{Input} $\textbf{b}=(b_i)_{0\leq i\leq m-1}\in k^m$, $P$ an irreducible
monic polynomial of degree $m$.\\
\textbf{Output} $(a_i)_{0\leq i \leq m-1}$ such that
$\tau_P(\sum_{0\leq i \leq m-1}ax^{i+j})=b_j$ for all $j$
\begin{enumerate}
  \item $S=1/P \mod P$
  \item $b=\rev(P,m+1)\sum_{0\leq i\leq m-1}b_ix^i\mod x^m$
  \item $c=\rev(b,m)$
  \item $d=cS \mod P$
  \item \textbf{return} (coefficient($d,x^i))_{0\leq i \leq m-1}$
\end{enumerate}

As we see, even if the lemma holds in $k[[x]]$, we just need to work $\mod x^m$
so the inverses can be obtained with fast algorithms for \textsc{Euclide}
division. \textbf{MonomialToDual} uses $O(M(m))$ operations in $k$, where 
$M(m)$ is the number of operations in $A$ of a multiplication of polynomials of 
degree at most $m$ on any ring $A$. The cost of \textbf{DualToMonomial} is 
$O(M(m)\log(m))$, but can be reduced to $O(M(m))$ with precomputations. $M$ is 
a function that is quasi-linear, so the change-of-basis algorithms are 
quasi-linaear too. 

\paragraph{Embedding and computing R.} Let $Q$ be an irreducible monic
polynomial of $k[y]$ of degree $n$, with $n$ coprime to $m$. Let also $R=P\odot
Q$ be the composed product of $P$ and $Q$. We have the extensions $k[x]/(P)$,
$k[y]/(Q)$, $k[x,y]/(P,Q)$, and $k[z]/(R)$ which respectively have the traces
$\tau_P$, $\tau_Q$, $\tau_{P,Q}$, $\tau_R$ and the associated bilinear forms.
These extensions are respectively endowed with the monomial bases $\bxi$,
$\bupsilon$, $\bxi\otimes\bupsilon$ and $\bzeta$. Finally, we denote by
$\bxi^*$, $\bupsilon^*$ and $\bzeta^*$ the dual basis of $\bxi$, $\bupsilon$ and
$\bzeta$ with respect to $\psdot_P$, $\psdot_Q$ and $\psdot_R$. In this paragraph, we will
give algorithms to compute the embeddings $\varphi_x$, $\varphi_y$, their
sections, and to compute the polynomial $R$. The algorithms follow from the next
lemma :
\begin{lm}
  \label{lm-trac}
  Let $b$ be in $k[x]/(P)$ and $c$ in $k[y]/(Q)$. Then we have
  $\tau_R(\Phi(bc))=\tau_{P,Q}(bc)=\tau_P(b)\tau_Q(c)$.
\end{lm}
This lemma will permit us to compute the restriction of $\Phi$ to the set 
\[
  \Pi=\left\{ bc \;|\; b\in k[x]/(P), c\in k[y]/(Q) \right\}\subset
  k[x,y]/(P,Q).
\]
Indeed, if we have the coordinates $(\tau_P(bx^i))$ and $(\tau_Q(cy^j))$ of $b$ and
$c$ in the dual bases, we can extend them up to $mn-1$ using $\remt$, then, by lemma $\ref{lm-trac}$, we have
$(\tau_P(bx^i)\tau_Q(cy^i))_{0\leq i \leq mn-1}=(\tau_R(\Phi(bc)z^i))_{0\leq i
\leq mn-1}$ which gives us the coordinates of $\Phi(bc)$ in the dual basis
$\bzeta^*$. So we can obtain $\varphi_x$ and $\varphi_y$ by applying the last
formula with respectively $c=1$ and $b=1$. This can be written as follows.
\newline

\textbf{Embed}(b, c, r)\\
\textbf{Input} $\textbf{b}=(b_i)_{0\leq i\leq m-1}\in k^m$,
$\textbf{c}=(c_i)_{0\leq i\leq n-1}\in k^n$, an optional integer $r\geq mn$ set
to $r=mn$ by defaut\\
\textbf{Output} $\textbf{a}=(a_i)_{0\leq i\leq r-1}\in k^r$
\begin{enumerate}
  \item $(t_i)_{0\leq i \leq r-1}=\remt(\textbf{b},P,r)$
  \item $(u_i)_{0\leq i \leq r-1}=\remt(\textbf{c},P,r)$
  \item \textbf{return} $(t_iu_i)_{0\leq i \leq r-1}$
\end{enumerate}
\textbf{Embed} uses $O(r(M(m)/m + M(n)/n))$ operations in $k$.
If $u_P$ is the vector of the coordinates of $1\in k[x]/(P)$ in the dual basis
$\bxi^*$ and $u_Q$ the vector define similarly, we can compute
$\varphi_x=\textbf{Embed}(\cdot,u_Q)$ and $\varphi_y=\textbf{Embed}(u_P,\cdot)$.
Since $R$ is the minimal polynomial of the sequence
$(\tau_R(z^i))=\textbf{Embed}(u_P,u_Q)$, we can obtain $R$ by computing the
\textsc{Berlekamp-Massey} algorithm (which compute minimal polynomials of linear
sequences).

As we have just seen, the theory of duality gives us a very simple algorithm for
\textbf{Embed}, but together with the transposition principle, it will also
prove usefull to find algorithms for the section of $\varphi_x$, and, as we will
see later, for $\Phi^{-1}$. Let $c\in k[y]/(Q)$ such that $\tau_Q(c)=1$ (for
example $c=\upsilon^*_0$ is convenient), $\varepsilon:k[x]/(P)\to k[z]/(R),
b\mapsto \Phi(bc)$, and $\varepsilon^t:k[z]/(R)\to k[x](P)$ its dual map with
respect to $\psdot_P$ and $\psdot_R$. Then, for any $b$, $b'$ in $k[x]/(P)$, we
have
\[
\begin{array}{rcl}
  \ps{b}{b'}_P &=& \tau_P(bb') = \tau_P(bb')\tau_Q(c) = \tau_R(\Phi(bb'c))\\
  &=& \ps{\varepsilon(b)}{\Phi(b')}_R = \ps{b}{\varepsilon^t(\Phi(b'))}_P.
\end{array}
\]
Since $\psdot_P$ is non-degenerate, this means that 
$b'=\varepsilon^t(\Phi(b'))=\varepsilon^t(\varphi_x(b'))$ for all $b'\in 
k[x]/(P)$. In other words, $\varepsilon^t$ is an inverse of $\varphi_x$ on its 
image, also called a section of $\varphi_x$. Remarking that $\varepsilon$ is 
nothing else that $\textbf{Embed}(\cdot,c)$, where $c=(1,0,\dots,0)$, we have 
an algorithm for $\varepsilon$, and, by transposing this algorithm, we have an 
algorithm for $\varepsilon^t$, which is the section of $\varphi_x$ we were 
searching. This algorithm uses $O(nM(m)+nM(n))$ operations in $k$.

\paragraph{Isomorphism.} We will apply the same kind of strategy to deduce an 
algorithm for $\Phi^{-1}$ from an algorithm for $\Phi$. First, recall that 
$\bxi\otimes\bupsilon$, $\bxi^*\otimes\bupsilon^*$, $\bxi^*\otimes\bupsilon$ 
and $\bxi\otimes\bupsilon^*$ are all bases of $k[x,y]/(P,Q)$ and that the 
second is dual to the first and the third to the fourth, with respect to 
$\psdot_{P,Q}$. Let $\Phi^t$ be the dual map to $\Phi$ with respect to 
$\psdot_{P,Q}$ and $\psdot_R$. For any $b$ and $b'$ in $k[x,y]/(P,Q)$, we have
\[
\begin{array}[here]{rcl}
  \ps{b}{b'}_{P,Q} &=& \tau_{P,Q}(bb') = \tau_R(\Phi(bb')) \\
  &=& \ps{\Phi(b)}{\Phi(b')}_R = \ps{b}{\Phi^t(\Phi(b'))}_{P,Q}.
\end{array}
\]
So, here again, thanks to the non-degeneracy of $\psdot_{P,Q}$, we have 
$\Phi^t=\Phi^{-1}$. Hence, if we have an algorithm computing $\Phi$ in  
some bases $\textbf{a}$ and $\textbf{b}$, we can deduce, using the 
transposition principle, an algorithm computing $\Phi^{-1}$ in the dual bases 
$\textbf{b}^*$ and $\textbf{a}^*$. We have two algorithms for computing $\Phi$, 
the first is better in cases where $m$ is small compared to $n$, and use the 
linearity of $\Phi$ and the work we already done. The idea is to express $b\in 
k[x,y]/(P,Q)$ in the form $b=\sum_i^{m-1}b_ix^i$ with $b_i\in k[y]/(Q)$ for all 
$0\leq i \leq m-1$. Since the $b_ix^i$ are all in $\Pi$, we can apply 
\textbf{Embed} and sum the result to obtain $\Phi(b)=\sum_i^{m-1}\Phi(b_ix^i)$. 
To apply this strategy, the $b_i$ must be expressed in the dual base 
$\bupsilon^*$ of $k[y]/(Q)$, because \textbf{Embed} takes its input in the dual 
bases. In the end, exploiting the fact that the coordinates of the $x^i$ in the 
dual basis $\bxi^*$ are $(\tau_P(x^{i+j}))_{0\leq j\leq m-1}$ ($u_P$ shifted 
$i$ times), we have an algorithm to compute $\Phi$ in the basis 
$\bxi\otimes\bupsilon^*$ and giving the result in the basis $\bzeta^*$. As we 
noticed, transposing this algorithm will create a new algorithm computing 
$\Phi^{-1}$ and expressed in the bases $\bzeta$ and $\bxi^*\otimes\bupsilon$. 
These algoithms both have a quadratic cost in the degree $m$ : $O(m^2M(n))$.

The second way of computing $\Phi$ does not depend on previous algorithms, but 
we will still be able to deduce an algorithm for $\Phi^{-1}$ from it using the 
transposition principle. It is best suited when dealing with $m$ and $n$ of the 
same magnitude and it uses baby steps / giant steps : a technique consisting of 
sacrificing memory to gain speed. Recall that $\Phi(xy)=z$, then, if $b\in 
k[x,y]/(P,Q)$, we can write
\begin{eqnarray*}
b&=&\sum_{i=0}^{m-1}\sum_{j=0}^{n-1} b_{i,j}x^i y^j
=~\sum_{i=0}^{m-1}\sum_{j=0}^{n-1} b_{i,j}x^i y^i y^{j-i}\\
&=&\sum_{h=-m+1}^{n-1}\sum_{i=0}^{m-1} b_{i,i+h}(xy)^i y^h
=~\frac{1}{y^{m-1}} \sum_{h=0}^{m+n-2} c_h(xy) y^h,
\end{eqnarray*}
with $c_h(xy)=\sum_{0 \leq i \leq m-1} b_{i,i+h-m+1} (xy)^i$, and where 
undefined indices $b_{i,i+h-m+1}$ equal zero. So $\Phi(xy)=z$ is known, but 
also $T=\Phi(y)$, because $y\in\Pi$ so we can use $\textbf{Embed}$ to compute 
it. Then, we have 
\[
a = \frac{1}{T^{m-1}}\tilde a \mod R \text{ with } \tilde a = \sum_{h=0}^{m+n-2} 
c_h(z) T^h.
\]
This is the equality that will be exploited to compute $\Phi$, and, thanks to 
baby steps / giant steps techniques, this will be reduced to matrix 
multiplication (but the coefficients of those matrices are polynomials). This 
technique gives an algorithm expressed in the bases $\bxi\otimes\bupsilon$ and 
$\bzeta$. Details can be found in \cite{DeDoSc14}. As we said earlyer, we 
immediatly deduce an algorithm for $\Phi^{-1}$ using the 
transposition principle, this last algorithm will be expressed in the bases 
$\bzeta^*$ and $\bxi^*\otimes\bupsilon^*$. Both these algorithms use 
$O(M(mn)n^{1/2}+M(m)n^{(\omega+1)/2})$ operations in $k$, where $\omega$ is a 
constant such that one can multiply two matrices of size $n\times n$ over any 
ring $A$ in $O(n^\omega)$ operations in $A$ (we know that $2 < \omega \leq 
2.38$).


\section{Julia/Nemo in practice}
Since Julia is easy-to-write, I hope it's also
easy-to-read, if not, I think that it is due to my lack of experience in
software programing, rather than to Julia itself. You can find the code
at
https://gitbub.com/edouardRousseau/FastArithmetic.jl. What's more, it is really
easy to install personal packages, you just have to clone my repository
inside Julia, using
\begin{verbatim}
julia> Pkg.clone(``https://github/edouardRousseau/FastArithmetic.jl'') 
\end{verbatim}
and you can also test the package easily by running the following.
\begin{verbatim}
julia> Pkg.test(``FastArithmetic'') 
\end{verbatim}
You may want to update the package, because the code is very suceptible to 
change, so just use
\begin{verbatim}
julia> Pkg.update()
\end{verbatim}
and you will be ready to start again. 
\subsection{Benchmarks}
We will now look at the speed of the code in Julia, and compare it with
the speed in C. All the benchmarks in Julia were realised with the
@benchmark macro available in the BenchmarkTools package. This macro lets you
benchmark any function with 

\begin{verbatim}
julia>@benchmark 1+1
BenchmarkTools.Trial: 
  samples:          10000
  evals/sample:     1000
  time tolerance:   5.00%
  memory tolerance: 1.00%
  memory estimate:  0.00 bytes
  allocs estimate:  0
  minimum time:     3.00 ns (0.00% GC)
  median time:      3.00 ns (0.00% GC)
  mean time:        3.04 ns (0.00% GC)
  maximum time:     10.00 ns (0.00% GC)
\end{verbatim}
which runs the desired function and measures the time it takes. It's possible
to choose all the parameters (number of samples, number of evaluation
per sample,\dots) of the benchmark and you can use different
estimators. For the following benchmarks, we use the median time, because it
is more stable than the mean time and still give an idea of the average time
the function will take. The stability we are talking about is linked with the
memory management in Julia. Indeed, in Julia, we don't have to concern about
memory, and the variables we assign are cleared by the Garbage Collector
(GC). The time Julia pass in the GC is not very clear nor predictible, and the
operations in the GC take a significant time. That's why the time taken by a
function can change between two samples, depending on the time spent in the GC.
By default, before every benchmark, the garbage is automaticaly collected, but
there is still a bit of unstabiliy.
Since all the functions written in FastArithmetic.jl depends on at least one
polynomial $P$, sometimes also $Q$ and $R=P\odot Q$, we let $m=\deg (P)$ grow
from $1$ to $200$ and benchmark the functions for all these values of $m$. We
choose $\deg (Q)=m+1$, and both $P$ and $Q$ are irreducible polynomials.
Irreducible polynomials of degree $1$ to $201$ were precomputed, by chosing
random elements in $\mathbb{F}_{p}[X]$, in order to
always use the same list and to be sure that we are not using special
polynomials that could be speeding up the functions (like sparse
polynomials). In the arrays, the speed of
the C code is set to $1.00$, and smaller is better. The speed of 
\textbf{monomialToDual} and of \textbf{dualToMonomial} are the same, either in C
or Julia, so we show the results only for the first.

\begin{center}
  \begin{tabular}[here]{c}
    \textbf{mulmod} \\
    \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $15.47$\\
   $51$ & $12.84$\\
   $104$ & $17.04$\\
   $140$ & $16.43$\\
   $187$ & $14.78$\\
    \end{tabular}
  \end{tabular}
\end{center}

\begin{center}
\begin{tabular}[here]{ccc}
\textbf{monomialToDual} & \textbf{mulModT} & \textbf{embed} \\

  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $5.12$\\
   $51$ & $1.14$\\
   $104$ & $0.56$\\
   $140$ & $0.44$\\
   $187$ & $0.38$\\

  \end{tabular}

  &
  
  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $286$\\
   $51$ & $2.6\times 10^3$\\
   $104$ & $6.8\times 10^3$\\
   $140$ & $1.17\times 10^4$\\
   $187$ & $2\times 10^4$\\

  \end{tabular}

  &

  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $31.30$\\
   $51$ & $37.17$\\
   $104$ & $35.29$\\
   $140$ & $33.87$\\
   $187$ & $30.87$\\

  \end{tabular}\\

    \textbf{project} & \textbf{phi1} & \textbf{phi2} \\

  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $32.77$\\
   $51$ & $30.64$\\
   $104$ & $26.64$\\
   $140$ & $24.24$\\
   $187$ & $20.64$\\

  \end{tabular}

  &
 
  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $25.83$\\
   $51$ & $29.55$\\
   $104$ & $22.53$\\
   $140$ & $22.94$\\
   $187$ & $22.12$\\

  \end{tabular}

  &
 
  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $2.56$\\
   $51$ & $102$\\
   $104$ & $187$\\
   $140$ & $309$\\
   $187$ & $449$\\

  \end{tabular}
\end{tabular}
\end{center}


\clearpage
\bibliographystyle{unsrt}
\bibliography{biblio}
\end{document}
