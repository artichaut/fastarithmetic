%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% # TO DO LIST
% 
% * drawing for the transposition principle
% * explainations of the transposition principle with examples (rev, rem etc.)
% * re-think the traces introduction to fit with bivariate conversions
% * explain R = P(*)Q also 
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[a4paper,11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath,amssymb,amsthm,amsopn}
\usepackage{mathrsfs}
%\usepackage{stmaryrd} % to get <--| \mapsfrom
\usepackage{graphicx}
\usepackage{hyperref}
%\usepackage{tikz}
\usepackage{array}
%\usepackage[top=1cm,bottom=1cm]{geometry}
%\usepackage{listings}
%\usepackage{xcolor}
\usepackage{bm}
\usepackage{bbm}

% Création des labels Théorème, Lemme, etc...

\newtheoremstyle{break}%
{}{}%
{\itshape}{}%
{\bfseries}{}%  % Note that final punctuation is omitted.
{\newline}{}

\theoremstyle{break}
\newtheorem{thm}{Théorème}[section]
\newtheorem{lm}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{cor}[thm]{Corollaire}

\theoremstyle{definition}
\newtheorem{defi}[thm]{Définition}
\newtheorem{ex}[thm]{Exemple}

\theoremstyle{remark}
\newtheorem{rem}[thm]{Remarque}

% Raccourcis pour les opérateurs mathématiques (les espaces avant-après sont
% modifiés pour mieux rentrer dans les codes mathématiques usuels)
% \deg already exists !!
\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Id}{Id}
\DeclareMathOperator{\Img}{Im}
\DeclareMathOperator{\Card}{Card}
\DeclareMathOperator{\Vect}{Vect}
\DeclareMathOperator{\rev}{rev}

\DeclareBoldMathCommand{\bxi}{\xi}
\DeclareBoldMathCommand{\bupsilon}{\upsilon}
\DeclareBoldMathCommand{\bzeta}{\zeta}

% Nouvelles commandes
\newcommand{\ps}[2]{\langle#1,#2\rangle}
\newcommand{\ent}[2]{[\![#1,#2]\!]}
\newcommand{\psdot}{\ps{\cdot}{\cdot}}

% opening
\title{Internship report}
\author{Édouard \textsc{Rousseau}\\Internship supervised by Luca \textsc{De 
Feo}}



\begin{document}

\maketitle

\begin{abstract}

  This internship took place during July and August 2016, the aim was to study
  the algorithms presented in \cite{DDS14}, and to implement them in Nemo
  \cite{Nemo}, a computer algebra package for the Julia \cite{Julia} programming
  language. This paper intends to compare the Julia and the C implementations of
  the code, in terms of speed, but also genericity and easyness to write and
  read such code.

\end{abstract}

\tableofcontents

\clearpage

\section{Julia}
\subsection{Overview of Julia's caracteristics}
Julia is a free and open-source, high-level programming language developed since 
2012, with dynamic
type system and high-performance. It is a compiled language, with a
\emph{just-in-time} (jit) compilation. It means that when you write a
function, e.g. 
\begin{verbatim}
julia> function myFunction(n)
       return n^3+n+2
       end
\end{verbatim}
your function will be compiled during your very first call. After this first
call, it will be much faster to run the compiled function
\begin{verbatim}
julia> myFunction(BigInt(2)^300)
\end{verbatim}
than the expression below.
\begin{verbatim}
julia> (BigInt(2)^300)^3 + BigInt(2)^300 + 2
\end{verbatim}
In order to optimize the code, the compiler creates a new function for each
type of input that you can think of when calling myFunction. For example, in the
code 
below, Julia will compile myFunction three times, which will result in
three functions, each one of them optimized for \textbf{Int64} (the default type
of $17$), \textbf{Float64}, or \textbf{BigInt}.
\begin{verbatim}
julia> myFunction(17)
4932
julia> myFunction(17.)
4932.0
julia> myFunction(BigInt(17))
4932
\end{verbatim}
Julia is a very easy-to-learn language, I had never seen Julia code before the 
internship, and I am not an expert in computer science, but the writing part of
the code was not the hardest. 
The code written in FastArithmetic.jl is
very generic, thanks to the type system of Julia, and works for fields
$\mathbb{F}_{p^n}$ with a small $p$ as well as large $p$. More importantly, the
\emph{jit} compiler creates a function for each kind of $p$. Indeed the elements
of $\mathbb{F}_{p^n}$ have type \textbf{fq\_nmod} for $p$ small or \textbf{fq}
for $p$ large, so one can write only one function and have two compiled
functions : one for each type, which will be optimized by the compiler for this 
type.

\subsection{Nemo}

Nemo is a computer algebra package of Julia, you can install it with 
\begin{verbatim}
julia> using Nemo
\end{verbatim}
and test it with the following.
\begin{verbatim}
julia> Pkg.test(``Nemo'')
\end{verbatim}
Nemo is based on C/C++ libraries such as Flint, Antic, Pari, etc. and is also
written in Julia. It provides a lot of features, and all the work of this
internship is based on Nemo. Indeed, we work with Nemo finite fields and Nemo
polynomials over these fields. \textbf{fq}, \textbf{fq\_nmod}, and
\textbf{fq\_nmod\_poly} (the type of the polynomials over $\mathbb{F}_{p^n}$) 
are
all Nemo types. You can learn more in Nemo's manual, which is very well
documented.

\section{A bit of theory}
\subsection{General background}
In most of the computer algebra system, like Sage, Magma, Flint, etc., it is 
possible to work with finite
fields and their algebraic closure. Let $p$ be a prime number and
$k=\mathbb{F}_p$ the field with $p$ elements, the algebraic closure of $k$ is
infinite and is $\bar k = \cup_{i\geq 0} \mathbb{F}_{p^i}$, dealing with $\bar
k$ means being able to compute the finite fields $\mathbb{F}_{p^i}$, and also
being able to embed small fields into large ones, or project elements of large
fields into smaller ones (when possible).

These algorithms often rely on linear algebra, and the cost is at least 
quadratic in
the degree of the extension you are dealing with. The algorithms presented in
\cite{DDS14} rely on polynomial arithmetic, and have a quasi-linear cost for 
most of the algorithms needed to
compute in the algebraic closure of a finite field. In this section, we will
introduce (briefly) the tools used in the algorithms, and explain how they work.
The most important result is the \emph{transposition principle}, which is an
algorithmic proposition that guarantees that if you have a linear algorithm
performing a matrix-vector product $v\mapsto Mv$, you can \emph{tranpose} it to
obtain an other algorithm, which has essentialy the same cost, performing the 
transposed matrix-vector product $v\mapsto
M^tv$. But we first need to talk about \emph{duality}, because the transposition
principle also change the base you are working with.
\subsection{Trace and duality}
Let $k$ be a finite field, and  $E,F$ be two $k$-vector spaces with the same
finite dimension $\dim E = \dim F < \infty$. Let also
$\ps{\cdot}{\cdot}:E\times F\rightarrow k$ be a non-degenerate bilinear form. 
Then,
for any basis $\bxi=(\xi_i)_i$ of $E$, their exists a unique basis
$\bxi^*=(\xi_j^*)_j$ of $F$
such that $\forall (i,j),\ps{\xi_i}{\xi_j^*}=\delta_{i,j}$ where $\delta_{i,j}$ 
is
the \textsc{Kronecker} symbol and $\delta_{i,j}=1$ if $i=j$ and 0 otherwise.
The base $\bxi^*$ is called the \emph{dual basis} of $\bxi$.
Equally, let $E',F'$ be two other $k$-vector spaces with $\dim E' = \dim F' <
\infty$ and $\ps{\cdot}{\cdot}':E'\times F'\rightarrow k$ an other 
non-degenerate
bilinear form. Then, for any linear map $u:E\rightarrow E'$, there exists a
unique map $u^t:F'\rightarrow F$ such that $\forall (a,b)\in E\times
F',\ps{u(a)}{b}'=\ps{a}{u^t(b)}$. This map is called the \emph{dual} map of $u$,
with respect to $\ps{\cdot}{\cdot}$ and $\ps{\cdot}{\cdot}'$.

It's now time to speak about the \emph{trace form}, it will be our candidate to 
be 
$\ps{\cdot}{\cdot}$, the non-degenerate bilinear form. Let $P$ be an irreducible
polynomial of $k[x]$ of degree $m$, $K=k[x]/(P)$ be a finite
extension of $k$, and $a\in K$. We denote by $\mu_a$ the application of
multiplication-by-a, $\mu_a:K\rightarrow K, b\mapsto ab$. $K$ is a $k$-vector
space of dimension $m$ and $\mu_a$ is an endomorphism of $K$. But, for all 
$a,b\in K$, we also denote $\tau_P(ab)$ to be the trace of the endomorphism 
$\mu_{ab}$, and we define
$\ps{a}{b}_P=\tau_P(ab)$. This defines a non-degenerate bilinear form on 
$K\times
K$. Thus, if $\bxi$ is a basis of $K$, one can obtain a dual basis $\bxi^*$ with
respect to the bilinear form $\ps{\cdot}{\cdot}_P$. Hence, if $a\in K$ has the
coordinates $(a_i)_i$ in the basis $\bxi^*$, these coordinates are given by
$a_i=\ps{\xi_i}{a}$, so $a=\sum a_i\xi_i^*=\sum\ps{\xi_i}{a}\xi_i^*$. This
formula will be important for the change-of-basis algorithms.

Let now $Q$ be an irreducible polynomial of $k[y]$ of degree $n$, with $n$
coprime to $m$, then $L=k[x,y]/(P,Q)\cong \mathbb{F}_{p^{mn}}$ is also a finite 
extension of $k$, of
degree $mn$. Since $L$ is a $k$-vector space (of dimension $mn$), it is again
possible to construct the trace $\tau_{P,Q}$ of an element of $L$, and we define 
the same way
a non-degenerate bilinear form $\psdot_{P,Q}$ on $L\times L$. If
$\bxi=(x^i)_{0\leq i\leq m-1}$ and $\bupsilon=(y^j)_{0\leq j \leq n-1}$ are the
monomial basis of respectively $k[x]/(P)$ and $k[y]/(Q)$, then
$\bxi\otimes\bupsilon=(x^iy^j)_{0\leq i \leq m-1,0\leq j \leq n-1}$ is the
cannonical monomial basis of $L$. But, if $\bxi^*$ and $\bupsilon^*$ are
respectively the dual basis of $\bxi$ with respect to $\tau_P$ and the dual
basis of $\bupsilon$ with respect to $\tau_Q$, $\bxi\otimes\bupsilon^*$, 
$\bxi^*\otimes\bupsilon$, and $\bxi^*\otimes\bupsilon^*$, are also bases of $L$.
What's more, $\bxi\otimes\bupsilon$ and  $\bxi^*\otimes\bupsilon^*$ are dual
with respect to $\tau_{P,Q}$, and so are $\bxi\otimes\bupsilon^*$ and
$\bxi^*\otimes\bupsilon$.

Constructing
$\mathbb{F}_{p^{mn}}$ this way leads to a bivariate reprentation of the
elements. We prefer an univariate representation, because the algorithm we have
in this case are more efficient, even for simple operations such as addition or
multiplication. Hence, we can represent $\mathbb{F}_{p^{mn}}$ by $k[z]/(R)$
where $R=P\odot Q$ is the composed product of $P$ and $Q$. It means that if
$(p_i)_{0\leq i \leq m-1}$ and $(q_j)_{0\leq j\leq n-1}$ are respectively the
roots of $P$ and $Q$ in a algebraic closure of $k$, $R$ is the polynomial whose
roots are the products $p_iq_j$. So we have embeddings $\varphi_x$, $\varphi_y$
and an isomorphism $\Phi$ of the form :
\[
\begin{array}{ccccc}
  &\varphi_x: & k[x]/(P) & \mapsto & k[z]/(R)\\[2mm]
  & \varphi_y: & k[y]/(Q) & \mapsto & k[z]/(R)\\[2mm]
  \text{and}& \Phi:&  A=k[x,y]/(P,Q) & \mapsto & k[z]/(R).
%  & &  xy & \mapsfrom & z.
\end{array}
\]
We will provide algorithms for $\varphi_x$, $\varphi_y$ and $\Phi$ in section
\ref{sec-algo}.

\subsection{Transposition principle}
Let, as in the previous section, $\bxi$ be a basis of $E$, and $\bxi^*$ the
basis of $F$ which is dual to $\bxi$. Let also $\bupsilon$ and $\bupsilon^*$ be 
a
basis of $E'$ and its dual basis of $F'$. If $M$ is the matrix of a linear map
$u:E\rightarrow E'$ in the
bases $(\bxi,\bupsilon)$, then the matrix of the linear map $u^t:F'\rightarrow 
F$ in the bases
$(\bupsilon^*,\bxi^*)$ is $M^t$. Given an algorithm to compute a linear map
$u:E\rightarrow E'$ in the bases $(\bxi,\bupsilon)$, the transposition principle
enable one to compute the dual map $u^t:F'\rightarrow F$ in the bases
$(\bupsilon^*,\bxi^*)$. The costs of the two algorithms only differ by a
constant. Roughly speaking, transposing an algorithm consist on transposing all
the subroutines and inverse their orders.
\begin{center}
  Maybe place here a tabular with drawing \& algorithmic result in both
  transposed and non-transposed mode. 
\end{center}
\subsection{The algorithms}
\label{sec-algo}
As we saw in the previous section, transposing an algorithm result in a change
of basis. Thus, it is important to be able to go from a basis to another. Here
again, $k$ is a finite field and, $P$ an irreducible monic polynomial of $k[x]$, 
and
$K=k[x]/(P)$ is a finite extension of $k$. We will be considering two bases of
$K$ : the monomial basis $\bxi=(x^i)_{0\leq i \leq m-1}$ (where $m$ is the
degree of $P$), and its dual basis $\bxi^*$ with respect to $\psdot_P$.

\paragraph{MonomialToDual and DualToMonomial.}The two
first algorithms allow us to juggle the monomial and the dual basis. They are
both based on a lemma which describe the generating series of the traces
$\tau_P(ax^i)_{0\leq i \leq m-1}$, where $a\in K$. 
\begin{lm}
  \label{lm-basis}
  Let $a\in K$ be an element of $K$, in the formal power series ring $k[[x]]$,
  we have:
  \[
    \sum_{i\geq 0}\tau_P(ax^i)x^i=\frac{\rev(P'a\mod P,m)}{\rev(P,m+1)}.
  \]
\end{lm}
Going from the monomial basis to the dual basis can be done by computing
$\rev(P'a\mod P,m)\mod x^m$, $(\rev(P,m+1)\mod x^m)^{-1}$, and the product of
them. You obtain, by lemma \ref{lm-basis} :
\[
  \frac{\rev(P'a\mod P,m)}{\rev(P,m+1)}\mod x^m=\sum_{i=0}^{m-1}\tau_P(ax^i)x^i.
\]
Since we said that
$(\tau_P(ax^i))_{0\leq i \leq m-1}$ are the coordinates of $a$ in the dual basis
$\bxi^*$, taking the coefficient of the polynomial
$\sum_{i=0}^{m-1}\tau_P(ax^i)x^i$ gives a solution to fo from the monomial basis
to the dual basis.

Obtaining the coefficients in the monomial basis $\bxi$
knowing the coefficients in the dual basis $\bxi^*$ is done using the same
equality, but starting from the other half. So, computing $\sum_{0\leq i \leq
m-1}\tau_P(ax^i)x^i \mod x^m$, $\rev(P,m+1)$, and multiplying them gives, by
lemma \ref{lm-basis} again : 
\[
\rev(P,m+1)\sum_{0\leq i \leq m-1}\tau_P(ax^i)x^i \mod x^m = \rev(P'a\mod P,m) 
\mod x^m.
\]
Then, applying $\rev(\cdot,m)$ and multiplying by $(P')^{-1}\mod P$ gives $a\mod
P$, so taking the coefficients of $a$ gives the coefficients in the monomial
basis $\bxi$.

As we see, even if the lemma holds in $k[[x]]$, we just need to work $\mod x^m$
so the inverses can be obtained with fast algorithms for \textsc{Euclide}
division.

\section{Julia/Nemo in practice}
Since Julia is easy-to-write, I hope it's also
easy-to-read, if not, I think that it is due to my lack of experience in
software programing, rather than to Julia itself. You can find the code
at
https://gitbub.com/edouardRousseau/FastArithmetic.jl. What's more, it is really
easy to install personal packages, you just have to clone my repository
inside Julia, using
\begin{verbatim}
julia> Pkg.clone(``https://github/edouardRousseau/FastArithmetic.jl'') 
\end{verbatim}
and you can also test the package easily by running the following.
\begin{verbatim}
julia> Pkg.test(``FastArithmetic'') 
\end{verbatim}
You may want to update the package, because the code is very suceptible to 
change, so just use
\begin{verbatim}
julia> Pkg.update()
\end{verbatim}
and you will be ready to start again. 
\subsection{Benchmarks}
We will now look at the speed of the code in Julia, and compare it with
the speed in C. All the benchmarks in Julia were realised with the
@benchmark macro available in the BenchmarkTools package. This macro lets you
benchmark any function with 

\begin{verbatim}
julia>@benchmark 1+1
BenchmarkTools.Trial: 
  samples:          10000
  evals/sample:     1000
  time tolerance:   5.00%
  memory tolerance: 1.00%
  memory estimate:  0.00 bytes
  allocs estimate:  0
  minimum time:     3.00 ns (0.00% GC)
  median time:      3.00 ns (0.00% GC)
  mean time:        3.04 ns (0.00% GC)
  maximum time:     10.00 ns (0.00% GC)
\end{verbatim}
which runs the desired function and measures the time it takes. It's possible
to choose all the parameters of the benchmark and you can use different
estimators. For the following benchmarks, we use the median time, because it
is more stable than the mean time and still give an idea of the average time
the function will take. The stability we are talking about is linked with the
memory management in Julia. Indeed, in Julia, we don't have to concern about
memory, and the variables we assign are cleared by the Garbage Collector
(GC). The time Julia pass in the GC is not very clear nor predictible, and the
operations in the GC take a significant time. That's why the time taken by a
function can change between two samples, depending on the time spent in the GC.
By default, before every benchmark, the garbage if automaticaly collected, but
there is still a bit of unstabiliy.
Since all the functions written in FastArithmetic.jl depends on at least one
polynomial $P$, sometimes also $Q$ and $R=P\odot Q$, we let $m=\deg (P)$ grow
from $1$ to $200$ and benchmark the functions for all these values of $m$. We
choose $\deg (Q)=m+1$, and both $P$ and $Q$ are irreducible polynomials.
Irreducible polynomials of degree $1$ to $201$ were precomputed, by chosing
random elements in $\mathbb{F}_{p}[X]$, in order to
always use the same list and to be sure that we are not using special
polynomials that could be speeding up the functions (like sparse
polynomials). In the arrays, the speed of
the C code is set to $1.00$, and smaller is better. The speed of 
\textbf{monomialToDual} and of \textbf{dualToMonomial} are the same, either in C
or Julia, so we show the results only for the first.

\begin{center}
  \begin{tabular}[here]{c}
    \textbf{mulmod} \\
    \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $15.47$\\
   $51$ & $12.84$\\
   $104$ & $17.04$\\
   $140$ & $16.43$\\
   $187$ & $14.78$\\
    \end{tabular}
  \end{tabular}
\end{center}

\begin{center}
\begin{tabular}[here]{ccc}
\textbf{monomialToDual} & \textbf{mulModT} & \textbf{embed} \\

  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $5.12$\\
   $51$ & $1.14$\\
   $104$ & $0.56$\\
   $140$ & $0.44$\\
   $187$ & $0.38$\\

  \end{tabular}

  &
  
  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $286$\\
   $51$ & $2.6\times 10^3$\\
   $104$ & $6.8\times 10^3$\\
   $140$ & $1.17\times 10^4$\\
   $187$ & $2\times 10^4$\\

  \end{tabular}

  &

  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $31.30$\\
   $51$ & $37.17$\\
   $104$ & $35.29$\\
   $140$ & $33.87$\\
   $187$ & $30.87$\\

  \end{tabular}\\

    \textbf{project} & \textbf{phi1} & \textbf{phi2} \\

  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $32.77$\\
   $51$ & $30.64$\\
   $104$ & $26.64$\\
   $140$ & $24.24$\\
   $187$ & $20.64$\\

  \end{tabular}

  &
 
  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $25.83$\\
   $51$ & $29.55$\\
   $104$ & $22.53$\\
   $140$ & $22.94$\\
   $187$ & $22.12$\\

  \end{tabular}

  &
 
  \begin{tabular}[here]{cc}
   $m$ & Julia \\
   $10$ & $2.56$\\
   $51$ & $102$\\
   $104$ & $187$\\
   $140$ & $309$\\
   $187$ & $449$\\

  \end{tabular}
\end{tabular}
\end{center}


\clearpage
\bibliographystyle{unsrt}
\bibliography{biblio}
\end{document}
